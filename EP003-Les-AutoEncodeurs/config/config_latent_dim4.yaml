# Autoencoder configuration for MNIST

# Model architecture
model:
  hidden_dims: [512, 256, 128]  # Hidden layer dimensions for encoder (decoder will be symmetric)
  latent_dim: 4                 # Dimension of latent space (2 or 3 for visualization)

# Training parameters
training:
  batch_size: 1024
  epochs: 500
  learning_rate: 0.001
  weight_decay: 0.0004
  vis_frequency: 50              # Visualize reconstructions every N epochs
  save_frequency: 50             # Save model checkpoint every N epochs

# Paths for saving models and results
paths:
  model_dir: './models'
  results_dir: './results'

# Visualization parameters
visualization:
  perplexity: 30                # For t-SNE if used
  n_neighbors: 15               # For UMAP if used
  plot_type: '3d'               # '2d' or '3d'
  n_samples: 2000               # Number of test samples to visualize
  point_size: 5                 # Size of points in scatter plot
  alpha: 0.7                    # Transparency of points
  create_animation: true        # Whether to create a rotating animation for 3D plot
  n_frames: 120                 # Number of frames for animation
  fps: 30                       # Frames per second for animation
  dpi: 100                      # DPI for saved images/animations

# Weights & Biases (wandb) configuration
wandb:
  project: 'encode'  # Project name in wandb
  entity: null                  # Username or team name (null for default)
  name: null                    # Run name (null for auto-generated)
  tags: ['autoencoder', 'mnist', 'en-code'] # Tags for the run
  notes: 'Autoencoder training on MNIST dataset' # Notes for the run
  mode: 'online'                # 'online', 'offline', or 'disabled'
  log_model: true               # Whether to log model checkpoints
  log_freq: 1                   # Log metrics every N epochs

